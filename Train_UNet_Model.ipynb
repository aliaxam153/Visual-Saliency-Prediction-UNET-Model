{"cells":[{"cell_type":"markdown","metadata":{"id":"vHNXE8UW770O"},"source":["**UNET- Semantic Segmentation Model Training Implementation**"]},{"cell_type":"markdown","metadata":{"id":"ftd9fn5T8DgB"},"source":["**Step 1:** Configure Dataset Filepaths & Load functions"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":68333,"status":"ok","timestamp":1702835664503,"user":{"displayName":"Ali Azam","userId":"11224395010900996548"},"user_tz":-300},"id":"Xae73yfUjVVv","outputId":"ffb2ea0a-e7c0-4132-9c9e-1146da2cc8da"},"outputs":[{"name":"stdout","output_type":"stream","text":["Path exists: /home/aazam/dev/Visual-Saliency-Prediction-UNET-Model/\n","Total Images in Dataset: 13426\n"]}],"source":["import os\n","import glob\n","\n","# Replace the \"home/username/dev/\" with your own directory path\n","work_dir= \"/home/aazam/dev\" + \"/Visual-Saliency-Prediction-UNET-Model/\"\n","# Verify image paths\n","if not os.path.exists(work_dir):\n","    print(f\"Path does not exist: {work_dir}\")\n","else:\n","    print(f\"Path exists: {work_dir}\")\n","\n","    # Create lists of image and mask file paths\n","    image_paths = glob.glob(work_dir + 'Dataset/Images/*.png')\n","    mask_paths = glob.glob(work_dir + 'Dataset/Masks/*.png')\n","    print('Total Images in Dataset:', len(image_paths))\n","\n","    # Misc Paths\n","    pretrain_modelpath = work_dir + \"Pre-trained_Model/\"\n","    logpath = work_dir + 'Logs'\n","    os.makedirs(pretrain_modelpath, exist_ok=True)\n","    os.makedirs(logpath, exist_ok=True)\n","\n","    # Create new name for Model which will be trained\n","    modelpath = pretrain_modelpath + 'unetmodel_v'+str(len(glob.glob(pretrain_modelpath + '/*.keras')))+'.keras'\n","\n","# Resizing images is optional, since dataset image size is 256x256x3\n","SIZE_X = 128\n","SIZE_Y = 128\n","n_classes = 1  # Number of classes for segmentation\n","# Define your parameters\n","batch_size = 32\n","epochs = 20\n","num_data = len(image_paths)\n"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["import os\n","os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\" # Indicates that TensorFlow's Keras API  will be used for training\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"  # specify which GPU(s) to be used\n","\n","import tensorflow as tf\n","from segmentation_models import Unet\n","from segmentation_models.losses import bce_jaccard_loss\n","from segmentation_models.metrics import iou_score\n","\n","# Restrict TensorFlow to only use the GPU\n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","if gpus:\n","  try:\n","    # Restrict TensorFlow to only use the first GPU\n","    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n","    # Set a memory limit\n","    config = tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)\n","    tf.config.experimental.set_virtual_device_configuration(gpus[0], [config])\n","\n","  except RuntimeError as e:\n","    # Visible devices must be set before GPUs have been initialized\n","    print(e)\n","\n","# Define the dataset loading and preprocessing function\n","def preprocess_stage(image_path, mask_path):\n","    # Load and preprocess image\n","    image = tf.io.read_file(image_path)\n","    image = tf.image.decode_png(image, channels=3)\n","    image = tf.image.resize(image, (SIZE_X, SIZE_Y))\n","    image = tf.image.convert_image_dtype(image, tf.float32)\n","\n","    # Load and preprocess mask\n","    mask = tf.io.read_file(mask_path)\n","    mask = tf.image.decode_png(mask, channels=1)\n","    mask = tf.image.resize(mask, (SIZE_X, SIZE_Y))\n","    mask = tf.math.round(mask / 255.0)\n","\n","    return image, mask\n","\n","# Adopted U-Net model architecture from \"segmentation_models\" library\n","# Utilized ResNet34 as the backbone, used initial weights from pretrained network ImageNet (Transfer Learning)\n","def unet_model(input_shape):\n","    model = Unet(backbone_name='resnet34', input_shape=input_shape, encoder_weights='imagenet', classes=1, activation='sigmoid')\n","    model.compile(optimizer='adam', loss=bce_jaccard_loss, metrics=[iou_score])\n","    return model\n"]},{"cell_type":"markdown","metadata":{},"source":["**Step 2:** Prepare & Split datasets to three batches sets (Training Set, Validation Set & Testing Set)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Load Dataset and prepare for training:\n","Total Dataset Images: 13426 \n","Train Split Images (60%) : 8055 \n","Validate Split Images (20%) : 2685 \n","Test Split Images (20%) : 2686\n"]}],"source":["# Calculate the number of samples in each set\n","train_split = 0.6\n","validate_split = 0.2\n","test_split = 1-(train_split + validate_split)\n","\n","num_train = int(train_split * num_data)\n","num_valid = int(validate_split * num_data)\n","\n","# Convert your lists into TensorFlow Datasets\n","dataset = tf.data.Dataset.from_tensor_slices((image_paths, mask_paths))\n","\n","# Shuffle the dataset\n","dataset = dataset.shuffle(len(image_paths), seed=42)\n","\n","# Split the dataset\n","dataset_train = dataset.take(num_train)\n","dataset_remaining = dataset.skip(num_train)\n","dataset_valid = dataset_remaining.take(num_valid)\n","dataset_test = dataset_remaining.skip(num_valid)\n","\n","print('Load Dataset and prepare for training:')\n","print(f\"Total Dataset Images: {num_data}\",\n","      f\"\\nTrain Split Images ({round(train_split*100)}%) : {len(dataset_train)}\",\n","      f\"\\nValidate Split Images ({round(validate_split*100)}%) : {len(dataset_valid)}\",\n","      f\"\\nTest Split Images ({round(test_split*100)}%) : {len(dataset_test)}\"\n","      )\n","\n","# Apply your loading and preprocessing function\n","dataset_train = dataset_train.map(preprocess_stage)\n","dataset_valid = dataset_valid.map(preprocess_stage)\n","dataset_test = dataset_test.map(preprocess_stage)\n","\n","# Batch the data\n","dataset_train = dataset_train.batch(batch_size)\n","dataset_valid = dataset_valid.batch(batch_size)\n","dataset_test = dataset_test.batch(batch_size)\n","\n","# Prefetch for performance\n","dataset_train = dataset_train.prefetch(buffer_size=tf.data.AUTOTUNE)\n","dataset_valid = dataset_valid.prefetch(buffer_size=tf.data.AUTOTUNE)\n","dataset_test = dataset_test.prefetch(buffer_size=tf.data.AUTOTUNE)"]},{"cell_type":"markdown","metadata":{},"source":["**Step 3:** Train & Validate the Unet Model"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":797018,"status":"ok","timestamp":1702823042360,"user":{"displayName":"Ali Azam","userId":"11224395010900996548"},"user_tz":-300},"id":"fuENlaI1jVVy","outputId":"0668de73-9f8a-4968-e25c-b92bd28d87cb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n"]},{"name":"stderr","output_type":"stream","text":["2024-06-04 23:28:39.213213: E tensorflow/core/util/util.cc:131] oneDNN supports DT_INT32 only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m136/252\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2:56\u001b[0m 2s/step - iou_score: 0.3945 - loss: 0.9082"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[23], line 20\u001b[0m\n\u001b[1;32m     13\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     14\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     15\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mTensorBoard(log_dir\u001b[38;5;241m=\u001b[39m logpath),\n\u001b[1;32m     16\u001b[0m     checkpointer\n\u001b[1;32m     17\u001b[0m ]\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdataset_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Plot training and validation accuracy\u001b[39;00m\n\u001b[1;32m     26\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:314\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    313\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 314\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    316\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1515\u001b[0m   )\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import matplotlib.pyplot as plt\n","from tensorflow import keras\n","\n","# Define model\n","input_shape = (SIZE_X, SIZE_Y, 3)  # Adjust based on the size of your images\n","model = unet_model(input_shape)\n","\n","# Model Checkpoint - save model after every epochs\n","# Save Best only - latest best model will not be overwritten\n","checkpointer = tf.keras.callbacks.ModelCheckpoint(modelpath,verbose=1,save_best_only=True)\n","\n","# Early Stopping - TensorBoard Callbacks\n","callbacks = [\n","    tf.keras.callbacks.EarlyStopping(patience=3,monitor='val_loss'),\n","    tf.keras.callbacks.TensorBoard(log_dir= logpath),\n","    checkpointer\n","]\n","\n","# Train the model\n","history = model.fit(dataset_train, \n","                    validation_data = dataset_valid, \n","                    epochs = epochs, \n","                    callbacks = callbacks)\n","\n","# Plot training and validation accuracy\n","plt.figure(figsize=(12, 4))\n","plt.subplot(1, 2, 1)\n","plt.plot(history.history['iou_score'], label='Training IoU')\n","plt.plot(history.history['val_iou_score'], label='Validation IoU')\n","plt.title('Training and Validation IoU')\n","plt.xlabel('Epoch')\n","plt.ylabel('IoU')\n","plt.legend()\n","# Plot training and validation loss\n","plt.subplot(1, 2, 2)\n","plt.plot(history.history['loss'], label='Training Loss')\n","plt.plot(history.history['val_loss'], label='Validation Loss')\n","plt.title('Training and Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.tight_layout()\n","plt.show()\n","print(f\"Trained Model has been stored:{modelpath}\")"]},{"cell_type":"markdown","metadata":{"id":"tRRn_scQub2a"},"source":["**Step 4:** Evaluate the trained model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5588,"status":"ok","timestamp":1702835694555,"user":{"displayName":"Ali Azam","userId":"11224395010900996548"},"user_tz":-300},"id":"85fupNOXnlh4","outputId":"8e485cbe-1173-4b80-e4fe-d51b364c7c53"},"outputs":[],"source":["import os\n","\n","from keras.models import load_model\n","from keras.utils import plot_model\n","# Directory containing your model files\n","model_directory = drive_dir + 'Trained Models/'\n","# List all files in the directory\n","model_files = [f for f in os.listdir(model_directory) if f.endswith(\".h5\")]\n","# Display the list of model files\n","print(\"Available model files:\")\n","for i, model_file in enumerate(model_files, start=1):\n","    print(f\"{i}. {model_file}\")\n","# Ask the user to select a model file\n","try:\n","    selected_index = int(input(\"\\nEnter the number corresponding to the model file you want to select: \")) - 1\n","    selected_model = model_files[selected_index]\n","    selected_model_path = os.path.join(model_directory, selected_model)\n","    print(f\"\\nSelected model: {selected_model_path}\")\n","except (ValueError, IndexError):\n","    print(\"Invalid selection. Please enter a valid number.\")\n","\n","# Load the Model\n","model = keras.models.load_model(selected_model_path, compile=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":24502,"status":"ok","timestamp":1702835945969,"user":{"displayName":"Ali Azam","userId":"11224395010900996548"},"user_tz":-300},"id":"OMjAngEk0rTs","outputId":"93e8f80b-6d7d-4442-9483-b00d19ba5981"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from PIL import Image\n","\n","# Function to create a mask from predicted values\n","def create_mask(pred_mask):\n","    pred_mask = tf.argmax(pred_mask, axis=-1)\n","    pred_mask = pred_mask[..., tf.newaxis]\n","    return pred_mask[0]\n","\n","rows = 5\n","dataset_samples = dataset_test\n","\n","# Set up the subplots\n","fig, axs = plt.subplots(rows, 3, figsize=(9, rows*3))\n","\n","for i, (image, mask, filepath) in enumerate(dataset_samples):\n","    # Assuming `model` is already defined and trained\n","    pred_mask = model.predict(image)\n","    pred_mask_np = create_mask(pred_mask).numpy()\n","\n","    image_path = filepath[0].numpy().decode('utf-8')\n","    # Load the image using PIL (Python Imaging Library)\n","    img = Image.open(image_path)\n","    target_size=(SIZE_X,SIZE_Y)\n","    # Resize the image\n","    img = img.resize(target_size)\n","    row = i % rows\n","    axs[row, 0].imshow(img)\n","    axs[row, 0].set_title(f'Image {i + 1}')\n","    axs[row, 0].axis('off')\n","    axs[row, 1].imshow(mask[0],cmap='gray')\n","    axs[row, 1].set_title('Ground Truth Mask')\n","    axs[row, 1].axis('off')\n","    axs[row, 2].imshow(pred_mask[0],cmap='gray')\n","    axs[row, 2].set_title('Predicted Mask')\n","    axs[row, 2].axis('off')\n","\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":278048,"status":"ok","timestamp":1702832411604,"user":{"displayName":"Ali Azam","userId":"11224395010900996548"},"user_tz":-300},"id":"n8jiGe08eqOH","outputId":"d34ff251-d37d-4b76-e217-48e938147cf6"},"outputs":[],"source":["import numpy as np\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","\n","# Function to create a confusion matrix\n","def create_confusion_matrix(model, dataset):\n","    true_labels = []\n","    predicted_labels = []\n","    for images, masks, _ in dataset:\n","        # Get model predictions\n","        predictions = model.predict(images)\n","        # Append flattened masks and predictions to the lists\n","        true_labels.append(tf.reshape(masks, [-1]).numpy())\n","        predicted_labels.append(tf.reshape(predictions, [-1]).numpy())\n","    # Concatenate the lists to create NumPy arrays\n","    true_labels = np.concatenate(true_labels)\n","    predicted_labels = np.concatenate(predicted_labels)\n","    # Use vectorized operations to create confusion matrix\n","    threshold = 0.5\n","    true_labels_binary = (true_labels > threshold).astype(int)\n","    predicted_labels_binary = (predicted_labels > threshold).astype(int)\n","    cm = confusion_matrix(true_labels_binary, predicted_labels_binary)\n","    return cm\n","# Get confusion matrix for dataset_test\n","confusion_mat = create_confusion_matrix(model, dataset_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":668},"executionInfo":{"elapsed":1167,"status":"ok","timestamp":1702833635880,"user":{"displayName":"Ali Azam","userId":"11224395010900996548"},"user_tz":-300},"id":"PVf9XCJUiKck","outputId":"c6d0e17e-a683-4e8f-997c-1668f48f9202"},"outputs":[],"source":["# Plot the confusion matrix\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(confusion_mat, annot=True, fmt='d', cmap='Blues', xticklabels=['0', '1'], yticklabels=['0', '1'])\n","plt.xlabel('Predicted')\n","plt.ylabel('True')\n","plt.title('Confusion Matrix')\n","plt.show()\n","# Extract true positives, true negatives, false positives, and false negatives from the confusion matrix\n","tn, fp, fn, tp = confusion_mat.ravel()\n","# Calculate metrics\n","accuracy = (tp + tn) / (tp + tn + fp + fn)\n","precision = tp / (tp + fp) if (tp + fp) > 0 else 0  # To avoid division by zero\n","recall = tp / (tp + fn) if (tp + fn) > 0 else 0  # To avoid division by zero\n","f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0  # To avoid division by zero\n","print('\\nTrained Model Results:')\n","print(f\"Accuracy: {round(accuracy*100)}%\")\n","print(f\"Precision:  {round(precision*100)}%\")\n","print(f\"Recall:  {round(recall*100)}%\")\n","print(f\"F1 Score: {round(f1*100)}%\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
